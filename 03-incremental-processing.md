# ğŸ”„ 3. Processamento Incremental de Dados

## 3.1 ACID Transactions e Delta Lake

O Delta Lake Ã© uma camada de armazenamento que traz transaÃ§Ãµes ACID (Atomicidade, ConsistÃªncia, Isolamento, Durabilidade) para o mundo do big data. Isso significa que operaÃ§Ãµes de leitura e escrita sÃ£o seguras, consistentes e recuperÃ¡veis, mesmo em caso de falhas. O transaction log do Delta Lake permite rollback, time travel e garante integridade dos dados.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Delta Lake   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ACID          â”‚
â”‚ Time Travel   â”‚
â”‚ Rollback      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Exemplo:
- OperaÃ§Ãµes de merge, update e delete em tabelas Delta sÃ£o ACID.
- Time travel e rollback sÃ£o possÃ­veis graÃ§as ao transaction log.

## 3.2 Tabelas e Metadados

No Databricks, os dados sÃ£o organizados em tabelas, que podem ser gerenciadas (controladas pelo Databricks) ou externas (armazenadas em local definido pelo usuÃ¡rio). Cada tabela possui dados (arquivos Parquet/Delta) e metadados (estrutura, histÃ³rico, permissÃµes).

Exemplo:
```sql
CREATE TABLE vendas_managed (id INT, valor DOUBLE) USING DELTA;
CREATE TABLE vendas_ext (id INT, valor DOUBLE) USING DELTA LOCATION '/mnt/dados/vendas';
```

## 3.3 Versionamento e HistÃ³rico

O Delta Lake mantÃ©m o histÃ³rico de todas as operaÃ§Ãµes realizadas em uma tabela, permitindo auditoria, rollback e time travel. Cada alteraÃ§Ã£o gera uma nova versÃ£o da tabela, que pode ser consultada a qualquer momento.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OperaÃ§Ã£o 1 â”‚-->| OperaÃ§Ã£o 2 â”‚-->| OperaÃ§Ã£o 3 â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
      |                |                |
   VersÃ£o 1         VersÃ£o 2         VersÃ£o 3
```

Exemplo:
```sql
DESCRIBE HISTORY vendas_managed;
SELECT * FROM vendas_managed VERSION AS OF 2;
```

## 3.4 OtimizaÃ§Ã£o e ManutenÃ§Ã£o

Para garantir performance e economia de recursos, o Delta Lake oferece comandos de otimizaÃ§Ã£o:
- **Z-Ordering:** organiza dados para acelerar queries multidimensionais.
- **Vacuum:** remove arquivos antigos e libera espaÃ§o.
- **Optimize:** compacta arquivos pequenos em arquivos maiores.

Exemplo:
```sql
OPTIMIZE vendas_managed ZORDER BY (cliente_id);
VACUUM vendas_managed RETAIN 168 HOURS;
```

## 3.5 CriaÃ§Ã£o e ModificaÃ§Ã£o de Tabelas

O Databricks permite criar tabelas a partir de SELECTs (CTAS), adicionar colunas geradas, documentar tabelas e sobrescrever dados de forma eficiente.

Exemplo:
```sql
CREATE TABLE vendas_top AS SELECT * FROM vendas WHERE valor > 1000;
ALTER TABLE vendas ADD COLUMN valor_com_imposto DOUBLE GENERATED ALWAYS AS (valor * 1.1);
COMMENT ON TABLE vendas IS 'Tabela de vendas processadas';
```

## 3.6 MERGE, COPY INTO e DLT

Essas operaÃ§Ãµes facilitam ingestÃ£o incremental e deduplicaÃ§Ã£o:
- **MERGE:** faz upsert/deduplicaÃ§Ã£o ao escrever.
- **COPY INTO:** carrega dados de arquivos externos sem duplicar.
- **DLT (Delta Live Tables):** pipelines declarativos para ingestÃ£o e transformaÃ§Ã£o.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Novos Dados  â”‚----->  â”‚   MERGE      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                               |
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚Tabela Destinoâ”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ArquivosExternosâ”‚----->â”‚   COPY INTO  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                               |
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚Tabela Destinoâ”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Exemplo:
```sql
MERGE INTO vendas_destino USING vendas_novas ON vendas_destino.id = vendas_novas.id
WHEN MATCHED THEN UPDATE SET valor = vendas_novas.valor
WHEN NOT MATCHED THEN INSERT *;

COPY INTO vendas_destino FROM '/mnt/novos_dados/' FILEFORMAT = CSV;
```

## 3.7 Delta Live Tables e Auto Loader

O Delta Live Tables (DLT) permite criar pipelines de dados declarativos, com validaÃ§Ã£o, monitoramento e automaÃ§Ã£o. O Auto Loader facilita a ingestÃ£o incremental e escalÃ¡vel de arquivos, suportando modos triggered (mais barato) e continuous (menor latÃªncia).

Exemplo:
```python
import dlt
@dlt.table
def bronze():
    return spark.read.format('cloudFiles').option('cloudFiles.format', 'csv').load('/mnt/bronze/')
```

## 3.8 Constraints e CDC

Constraints garantem integridade dos dados (PK, FK, NOT NULL). O CDC (Change Data Capture) permite capturar e aplicar mudanÃ§as em tabelas, facilitando integraÃ§Ãµes e auditoria.

Exemplo:
```sql
ALTER TABLE clientes ADD CONSTRAINT pk_id PRIMARY KEY (id) ENFORCED;
APPLY CHANGES INTO clientes_destino FROM changes_source;
```

## 3.9 Auditoria e Troubleshooting

O event log do Databricks permite consultar mÃ©tricas, auditoria e lineage de pipelines. Para troubleshooting, Ã© possÃ­vel identificar notebooks com erro, uso de LIVE e STREAM, e analisar logs detalhados.

Exemplo:
```sql
SELECT * FROM event_log('/pipelines/<pipeline_id>') WHERE level = 'ERROR';
``` 